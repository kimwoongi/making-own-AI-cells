{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4 나만의 데이터 분류기.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "MJvVnvirY-xa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 공사중"
      ]
    },
    {
      "metadata": {
        "id": "JI6I_EpJfxTN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "index2name = {\n",
        "    0: '마카롱',\n",
        "    1: '티라미수',\n",
        "    2: '타르트'\n",
        "}\n",
        "\n",
        "몇번학습 = 100\n",
        "학습률 = 1e-5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P6PZeQhLc_cJ",
        "colab_type": "code",
        "outputId": "2631cc5a-114d-4bb3-ca94-f432bdcb1f86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "# drive.mount('/content/gdrive', force_remount=True)\n"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NRcSmDxzfLjY",
        "colab_type": "code",
        "outputId": "9a869895-81c6-4e7e-b04c-086d259648c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "train_path = './gdrive/My Drive/Colab Notebooks/myai/train (1)/'\n",
        "test_path = './gdrive/My Drive/Colab Notebooks/myai/test (1)/'\n",
        "train_files = os.listdir(train_path)\n",
        "test_files = os.listdir(test_path)\n",
        "\n",
        "x = [cv2.imread(train_path + file) for file in train_files]\n",
        "x = [cv2.cvtColor(each, cv2.COLOR_BGR2RGB) / 255 for each in x]\n",
        "x = np.asarray([cv2.resize(each, (32, 32), interpolation=cv2.INTER_CUBIC) for each in x])\n",
        "\n",
        "x_test = [cv2.imread(test_path + file) for file in test_files]\n",
        "x_test = [cv2.cvtColor(each, cv2.COLOR_BGR2RGB) / 255for each in x_test]\n",
        "x_test = np.asarray([cv2.resize(each, (32, 32), interpolation=cv2.INTER_CUBIC) for each in x_test])\n",
        "\n",
        "y = [int(file.split('_')[0]) for file in train_files] \n",
        "y_test = [int(file.split('_')[0]) for file in test_files] \n",
        "num_classes = len(index2name)\n",
        "eye = np.eye(num_classes)\n",
        "y = np.asarray([eye[each] for each in y])\n",
        "y_test = np.asarray([eye[each] for each in y_test])\n",
        "\n",
        "print('num_classes:', num_classes)\n",
        "print('x_shape:', x.shape)\n",
        "print('y_shape:', y.shape)\n"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_classes: 3\n",
            "x_shape: (30, 32, 32, 3)\n",
            "y_shape: (30, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Cu9Rlc17Wfiz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8c524905-04fd-4e05-b3cf-c316f2d90589"
      },
      "cell_type": "code",
      "source": [
        "# ls './gdrive/My Drive/Colab Notebooks/myai/'"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34m'test (1)'\u001b[0m/  \u001b[01;34m'train (1)'\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l2zC9DkNPKu4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# x = [cv2.imread(train_path + file) for file in train_files]\n",
        "# x_height = np.array([each.shape[0] for each in x])\n",
        "# x_width = np.array([each.shape[1] for each in x])\n",
        "# x_min_wh = np.array([min(x_height[i], x_width[i]) for i in range(len(x))])\n",
        "# p_h_min = x_height//2 - x_min_wh//2\n",
        "# p_h_max = x_height//2 + x_min_wh//2\n",
        "# p_w_min = x_width//2 - x_min_wh//2\n",
        "# p_w_max = x_width//2 + x_min_wh//2\n",
        "# x = [each[p_h_min[i]:p_h_max[i], p_w_min[i]:p_w_max[i]] / 127.5 - 1 for i, each in enumerate(x)]\n",
        "# x = np.asarray([cv2.resize(each, (100, 100), interpolation=cv2.INTER_CUBIC) for each in x])\n",
        "\n",
        "# x_test = [cv2.imread(test_path + file) for file in test_files]\n",
        "# x_height = np.array([each.shape[0] for each in x_test])\n",
        "# x_width = np.array([each.shape[1] for each in x_test])\n",
        "# x_min_wh = np.array([min(x_height[i], x_width[i]) for i in range(len(x_test))])\n",
        "# p_h_min = x_height//2 - x_min_wh//2\n",
        "# p_h_max = x_height//2 + x_min_wh//2\n",
        "# p_w_min = x_width//2 - x_min_wh//2\n",
        "# p_w_max = x_width//2 + x_min_wh//2\n",
        "# x_test = [each[p_h_min[i]:p_h_max[i], p_w_min[i]:p_w_max[i]] / 127.5 - 1 for i, each in enumerate(x_test)]\n",
        "# x_test = np.asarray([cv2.resize(each, (100, 100), interpolation=cv2.INTER_CUBIC) for each in x_test])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JGO805Igq4v8",
        "colab_type": "code",
        "outputId": "9a4fee73-48c1-40b1-d81b-b3f52f1f91f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1802
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Flatten, Dense\n",
        "from keras.optimizers import Adam, SGD\n",
        "\n",
        "act = 'tanh'\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, 3, padding='same', input_shape=(32, 32, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(act))\n",
        "model.add(Conv2D(64, 3, padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(act))\n",
        "model.add(MaxPool2D())\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(act))\n",
        "model.add(Conv2D(128, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(act))\n",
        "model.add(MaxPool2D())\n",
        "\n",
        "model.add(Conv2D(256, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(act))\n",
        "model.add(Conv2D(256, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(act))\n",
        "model.add(Conv2D(256, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(act))\n",
        "model.add(MaxPool2D())\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(act))\n",
        "model.add(Conv2D(512, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(act))\n",
        "model.add(Conv2D(512, (3, 3), padding='same'))\n",
        "model.add(MaxPool2D())\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(act))\n",
        "model.add(Conv2D(512, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(act))\n",
        "model.add(Conv2D(512, (3, 3), padding='same'))\n",
        "model.add(MaxPool2D())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(act))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "adam = Adam(lr=학습률)\n",
        "model.compile(adam, 'categorical_crossentropy', ['accuracy'])\n",
        "model.summary()\n"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_92 (Conv2D)           (None, 32, 32, 64)        1792      \n",
            "_________________________________________________________________\n",
            "batch_normalization_85 (Batc (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_85 (Activation)   (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_93 (Conv2D)           (None, 32, 32, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_86 (Batc (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_86 (Activation)   (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_36 (MaxPooling (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_94 (Conv2D)           (None, 16, 16, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_87 (Batc (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_87 (Activation)   (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_95 (Conv2D)           (None, 16, 16, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_88 (Batc (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_88 (Activation)   (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_37 (MaxPooling (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_96 (Conv2D)           (None, 8, 8, 256)         295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_89 (Batc (None, 8, 8, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_89 (Activation)   (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_97 (Conv2D)           (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_90 (Batc (None, 8, 8, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_90 (Activation)   (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_98 (Conv2D)           (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_91 (Batc (None, 8, 8, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_91 (Activation)   (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_38 (MaxPooling (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_99 (Conv2D)           (None, 4, 4, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "batch_normalization_92 (Batc (None, 4, 4, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_92 (Activation)   (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_100 (Conv2D)          (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_93 (Batc (None, 4, 4, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_93 (Activation)   (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_101 (Conv2D)          (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_39 (MaxPooling (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_102 (Conv2D)          (None, 2, 2, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_94 (Batc (None, 2, 2, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_94 (Activation)   (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_103 (Conv2D)          (None, 2, 2, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_95 (Batc (None, 2, 2, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_95 (Activation)   (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_104 (Conv2D)          (None, 2, 2, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_40 (MaxPooling (None, 1, 1, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "batch_normalization_96 (Batc (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "activation_96 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 3)                 1539      \n",
            "=================================================================\n",
            "Total params: 14,993,731\n",
            "Trainable params: 14,986,307\n",
            "Non-trainable params: 7,424\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3qbxeTYUf32l",
        "colab_type": "code",
        "outputId": "98358529-cbca-41b4-913d-1352a16c4d42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3735
        }
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(x, y, epochs=몇번학습, validation_split = 0.1)\n"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 27 samples, validate on 3 samples\n",
            "Epoch 1/100\n",
            "27/27 [==============================] - 8s 308ms/step - loss: 1.3534 - acc: 0.2963 - val_loss: 0.4324 - val_acc: 1.0000\n",
            "Epoch 2/100\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.1627 - acc: 1.0000 - val_loss: 0.3125 - val_acc: 1.0000\n",
            "Epoch 3/100\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.0273 - acc: 1.0000 - val_loss: 0.2466 - val_acc: 1.0000\n",
            "Epoch 4/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0097 - acc: 1.0000 - val_loss: 0.2036 - val_acc: 1.0000\n",
            "Epoch 5/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.1740 - val_acc: 1.0000\n",
            "Epoch 6/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.1526 - val_acc: 1.0000\n",
            "Epoch 7/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.1366 - val_acc: 1.0000\n",
            "Epoch 8/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.1241 - val_acc: 1.0000\n",
            "Epoch 9/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.1137 - val_acc: 1.0000\n",
            "Epoch 10/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 9.8291e-04 - acc: 1.0000 - val_loss: 0.1051 - val_acc: 1.0000\n",
            "Epoch 11/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 8.3089e-04 - acc: 1.0000 - val_loss: 0.0979 - val_acc: 1.0000\n",
            "Epoch 12/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 7.2041e-04 - acc: 1.0000 - val_loss: 0.0917 - val_acc: 1.0000\n",
            "Epoch 13/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 6.3673e-04 - acc: 1.0000 - val_loss: 0.0865 - val_acc: 1.0000\n",
            "Epoch 14/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 5.7136e-04 - acc: 1.0000 - val_loss: 0.0820 - val_acc: 1.0000\n",
            "Epoch 15/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 5.1955e-04 - acc: 1.0000 - val_loss: 0.0783 - val_acc: 1.0000\n",
            "Epoch 16/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 4.7774e-04 - acc: 1.0000 - val_loss: 0.0752 - val_acc: 1.0000\n",
            "Epoch 17/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 4.4330e-04 - acc: 1.0000 - val_loss: 0.0725 - val_acc: 1.0000\n",
            "Epoch 18/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 4.1437e-04 - acc: 1.0000 - val_loss: 0.0701 - val_acc: 1.0000\n",
            "Epoch 19/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 3.8969e-04 - acc: 1.0000 - val_loss: 0.0680 - val_acc: 1.0000\n",
            "Epoch 20/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 3.6837e-04 - acc: 1.0000 - val_loss: 0.0662 - val_acc: 1.0000\n",
            "Epoch 21/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 3.4981e-04 - acc: 1.0000 - val_loss: 0.0646 - val_acc: 1.0000\n",
            "Epoch 22/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 3.3352e-04 - acc: 1.0000 - val_loss: 0.0632 - val_acc: 1.0000\n",
            "Epoch 23/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 3.1916e-04 - acc: 1.0000 - val_loss: 0.0620 - val_acc: 1.0000\n",
            "Epoch 24/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 3.0638e-04 - acc: 1.0000 - val_loss: 0.0609 - val_acc: 1.0000\n",
            "Epoch 25/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 2.9493e-04 - acc: 1.0000 - val_loss: 0.0598 - val_acc: 1.0000\n",
            "Epoch 26/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 2.8455e-04 - acc: 1.0000 - val_loss: 0.0589 - val_acc: 1.0000\n",
            "Epoch 27/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 2.7510e-04 - acc: 1.0000 - val_loss: 0.0581 - val_acc: 1.0000\n",
            "Epoch 28/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 2.6654e-04 - acc: 1.0000 - val_loss: 0.0574 - val_acc: 1.0000\n",
            "Epoch 29/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 2.5873e-04 - acc: 1.0000 - val_loss: 0.0567 - val_acc: 1.0000\n",
            "Epoch 30/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 2.5151e-04 - acc: 1.0000 - val_loss: 0.0561 - val_acc: 1.0000\n",
            "Epoch 31/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 2.4488e-04 - acc: 1.0000 - val_loss: 0.0555 - val_acc: 1.0000\n",
            "Epoch 32/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 2.3872e-04 - acc: 1.0000 - val_loss: 0.0550 - val_acc: 1.0000\n",
            "Epoch 33/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 2.3299e-04 - acc: 1.0000 - val_loss: 0.0546 - val_acc: 1.0000\n",
            "Epoch 34/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 2.2764e-04 - acc: 1.0000 - val_loss: 0.0542 - val_acc: 1.0000\n",
            "Epoch 35/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 2.2263e-04 - acc: 1.0000 - val_loss: 0.0538 - val_acc: 1.0000\n",
            "Epoch 36/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 2.1794e-04 - acc: 1.0000 - val_loss: 0.0535 - val_acc: 1.0000\n",
            "Epoch 37/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 2.1356e-04 - acc: 1.0000 - val_loss: 0.0532 - val_acc: 1.0000\n",
            "Epoch 38/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 2.0941e-04 - acc: 1.0000 - val_loss: 0.0529 - val_acc: 1.0000\n",
            "Epoch 39/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 2.0550e-04 - acc: 1.0000 - val_loss: 0.0527 - val_acc: 1.0000\n",
            "Epoch 40/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 2.0180e-04 - acc: 1.0000 - val_loss: 0.0525 - val_acc: 1.0000\n",
            "Epoch 41/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.9828e-04 - acc: 1.0000 - val_loss: 0.0523 - val_acc: 1.0000\n",
            "Epoch 42/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.9495e-04 - acc: 1.0000 - val_loss: 0.0521 - val_acc: 1.0000\n",
            "Epoch 43/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.9179e-04 - acc: 1.0000 - val_loss: 0.0519 - val_acc: 1.0000\n",
            "Epoch 44/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.8878e-04 - acc: 1.0000 - val_loss: 0.0518 - val_acc: 1.0000\n",
            "Epoch 45/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.8588e-04 - acc: 1.0000 - val_loss: 0.0516 - val_acc: 1.0000\n",
            "Epoch 46/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.8314e-04 - acc: 1.0000 - val_loss: 0.0515 - val_acc: 1.0000\n",
            "Epoch 47/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.8050e-04 - acc: 1.0000 - val_loss: 0.0513 - val_acc: 1.0000\n",
            "Epoch 48/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.7796e-04 - acc: 1.0000 - val_loss: 0.0512 - val_acc: 1.0000\n",
            "Epoch 49/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.7552e-04 - acc: 1.0000 - val_loss: 0.0511 - val_acc: 1.0000\n",
            "Epoch 50/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.7319e-04 - acc: 1.0000 - val_loss: 0.0510 - val_acc: 1.0000\n",
            "Epoch 51/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.7094e-04 - acc: 1.0000 - val_loss: 0.0509 - val_acc: 1.0000\n",
            "Epoch 52/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.6877e-04 - acc: 1.0000 - val_loss: 0.0508 - val_acc: 1.0000\n",
            "Epoch 53/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.6668e-04 - acc: 1.0000 - val_loss: 0.0508 - val_acc: 1.0000\n",
            "Epoch 54/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.6467e-04 - acc: 1.0000 - val_loss: 0.0507 - val_acc: 1.0000\n",
            "Epoch 55/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.6270e-04 - acc: 1.0000 - val_loss: 0.0506 - val_acc: 1.0000\n",
            "Epoch 56/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.6081e-04 - acc: 1.0000 - val_loss: 0.0506 - val_acc: 1.0000\n",
            "Epoch 57/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.5899e-04 - acc: 1.0000 - val_loss: 0.0505 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.5720e-04 - acc: 1.0000 - val_loss: 0.0504 - val_acc: 1.0000\n",
            "Epoch 59/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.5546e-04 - acc: 1.0000 - val_loss: 0.0504 - val_acc: 1.0000\n",
            "Epoch 60/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.5379e-04 - acc: 1.0000 - val_loss: 0.0503 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.5214e-04 - acc: 1.0000 - val_loss: 0.0503 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.5056e-04 - acc: 1.0000 - val_loss: 0.0502 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.4901e-04 - acc: 1.0000 - val_loss: 0.0502 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.4748e-04 - acc: 1.0000 - val_loss: 0.0501 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.4601e-04 - acc: 1.0000 - val_loss: 0.0501 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.4459e-04 - acc: 1.0000 - val_loss: 0.0501 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.4317e-04 - acc: 1.0000 - val_loss: 0.0500 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.4180e-04 - acc: 1.0000 - val_loss: 0.0500 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.4046e-04 - acc: 1.0000 - val_loss: 0.0500 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.3914e-04 - acc: 1.0000 - val_loss: 0.0499 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.3786e-04 - acc: 1.0000 - val_loss: 0.0499 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.3660e-04 - acc: 1.0000 - val_loss: 0.0498 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.3538e-04 - acc: 1.0000 - val_loss: 0.0498 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.3416e-04 - acc: 1.0000 - val_loss: 0.0498 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.3298e-04 - acc: 1.0000 - val_loss: 0.0497 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.3182e-04 - acc: 1.0000 - val_loss: 0.0497 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.3068e-04 - acc: 1.0000 - val_loss: 0.0497 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.2956e-04 - acc: 1.0000 - val_loss: 0.0497 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.2847e-04 - acc: 1.0000 - val_loss: 0.0496 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.2739e-04 - acc: 1.0000 - val_loss: 0.0496 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.2634e-04 - acc: 1.0000 - val_loss: 0.0496 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.2529e-04 - acc: 1.0000 - val_loss: 0.0495 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.2427e-04 - acc: 1.0000 - val_loss: 0.0495 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.2325e-04 - acc: 1.0000 - val_loss: 0.0495 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.2227e-04 - acc: 1.0000 - val_loss: 0.0495 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.2130e-04 - acc: 1.0000 - val_loss: 0.0494 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.2034e-04 - acc: 1.0000 - val_loss: 0.0494 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.1940e-04 - acc: 1.0000 - val_loss: 0.0494 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.1847e-04 - acc: 1.0000 - val_loss: 0.0494 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.1755e-04 - acc: 1.0000 - val_loss: 0.0493 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.1665e-04 - acc: 1.0000 - val_loss: 0.0493 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.1576e-04 - acc: 1.0000 - val_loss: 0.0493 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.1488e-04 - acc: 1.0000 - val_loss: 0.0493 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.1402e-04 - acc: 1.0000 - val_loss: 0.0492 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.1318e-04 - acc: 1.0000 - val_loss: 0.0492 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.1233e-04 - acc: 1.0000 - val_loss: 0.0492 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.1150e-04 - acc: 1.0000 - val_loss: 0.0492 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.1069e-04 - acc: 1.0000 - val_loss: 0.0491 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.0990e-04 - acc: 1.0000 - val_loss: 0.0491 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.0911e-04 - acc: 1.0000 - val_loss: 0.0491 - val_acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Vy1XXqBShs4_",
        "colab_type": "code",
        "outputId": "5b195cf1-08bf-4c97-f301-ba4c0c9196cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(x_test)\n",
        "y_pred2 = np.argmax(y_pred, axis=1)\n",
        "y_test2 = np.argmax(y_test, axis=1)\n",
        "\n",
        "y_pred_names = [index2name[y_pred2[i]] for i in range(len(y_pred2))]\n",
        "y_test_names = [index2name[y_test2[i]] for i in range(len(y_test2))]\n",
        "\n",
        "count = 0\n",
        "total = len(y_test_names)\n",
        "for i, (y_pred_name, y_test_name) in enumerate(zip(y_pred_names, y_test_names)):\n",
        "  if y_pred_name == y_test_name:\n",
        "    count += 1\n",
        "accuracy = count / total * 100\n",
        "\n",
        "print('정답들:', y_test_names)\n",
        "print('예측들:', y_pred_names)\n",
        "print('정확도:', accuracy)\n"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "정답들: ['타르트', '타르트', '티라미수', '마카롱', '마카롱', '티라미수']\n",
            "예측들: ['타르트', '타르트', '티라미수', '타르트', '마카롱', '티라미수']\n",
            "정확도: 83.33333333333334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SdTsNzQLdYEd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "5a702041-c272-4975-d1d0-4790ef7c98f6"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "which_file_index = 3\n",
        "image = x_test\n",
        "label = y_pred_names\n",
        "\n",
        "print(label[which_file_index])\n",
        "plt.imshow(image[which_file_index])\n",
        "plt.show()\n"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "타르트\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}